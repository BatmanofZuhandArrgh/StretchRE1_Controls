{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33aa1904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Ready\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import tkinter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# matplotlib.use('TkAgg')\n",
    "# %matplotlib widget\n",
    "%matplotlib inline \n",
    "from PIL import Image\n",
    "import IPython\n",
    "\n",
    "import pyrealsense2 as rs                 # Intel RealSense cross-platform open-source API\n",
    "from stretch_body.robot import Robot\n",
    "from scipy.spatial.transform import Rotation\n",
    "from re1_utils.camera import get_cur_rs_frame\n",
    "from re1_utils.misc import reset_head_position\n",
    "from POI.point_of_interest import POI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844080a",
   "metadata": {},
   "source": [
    "**Object Detection**: From yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7527bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#Gotta stay in the yolov7 directory\n",
    "sys.path.append('./yolov7/')\n",
    "from yolov7.predict import load_model, load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32274f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [utils.torch_utils]: YOLOR ðŸš€ a0bbdb1 torch 1.12.1+cu102 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [utils.torch_utils]: Model Summary: 306 layers, 36905341 parameters, 36905341 gradients\n"
     ]
    }
   ],
   "source": [
    "#Load config from yaml\n",
    "CONFIG_PATH = './yolov7/predict_config.yaml'\n",
    "config_dict = load_config(CONFIG_PATH)\n",
    "\n",
    "#Load model, edit yolov7.pt path, get model/stride/device from model config\n",
    "model, stride, device = load_model(config_dict, '../yolov7/yolov7.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d0fce2",
   "metadata": {},
   "source": [
    "Read sample img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a15688",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_frame, color, depth_frame, depth = get_cur_rs_frame()\n",
    "IPython.display.display(Image.fromarray(color))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9429a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re1_utils.objdet_utils import plot_all_boxes, predict\n",
    "\n",
    "pred = predict(model, config_dict, color, stride, device)\n",
    "print(pred)\n",
    "#xyxy, conf_score, class\n",
    "color = np.ascontiguousarray(color, dtype=np.uint8)\n",
    "plot_all_boxes(pred, color)\n",
    "IPython.display.display(Image.fromarray(color))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acba000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re1_utils.camera import get_rs_colorized_depth\n",
    "colorized_depth = get_rs_colorized_depth(depth_frame=depth_frame)\n",
    "\n",
    "IPython.display.display(Image.fromarray(colorized_depth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0647fef5-713c-4221-a8d3-1dcb2afeb8a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "bad magic number in 're1_utils.controls': b'\\x03\\xf3\\r\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre1_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrols\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m move_head \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m radians\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre1_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcamera\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cur_rs_frame\n",
      "\u001b[0;31mImportError\u001b[0m: bad magic number in 're1_utils.controls': b'\\x03\\xf3\\r\\n'"
     ]
    }
   ],
   "source": [
    "from re1_utils.controls import move_head \n",
    "from math import radians\n",
    "from re1_utils.camera import get_cur_rs_frame\n",
    "from POI.landmark_screen import LandmarkScreen\n",
    "from re1_utils.camera import get_rs_intrinsic_mat\n",
    "from re1_utils.camera import get_rs_extrinsic_mat\n",
    "\n",
    "\n",
    "def pre_grasping_perception_singleObj(sample_size = 5):\n",
    "    reset_head_position()\n",
    "    #Return sampled measurement of the FIRST object \n",
    "    world_coords = np.zeros(3)\n",
    "    \n",
    "    #Sampling the measurements multiple times, at the same camera angle 0 degrees\n",
    "    for i in range(sample_size): #range(-15,15,5):\n",
    "        # Tilt camera to for multiple measurement from different angles. As we discussed, this unecessarily produces errors with coordinate mapping and depth\n",
    "        # move_head('head_tilt',radians(i) )\n",
    "\n",
    "        #Get frame\n",
    "        color_frame, color, depth_frame, depth = get_cur_rs_frame()\n",
    "        \n",
    "        #Visualize\n",
    "        # color = np.ascontiguousarray(color, dtype=np.uint8)\n",
    "        pred = predict(model, config_dict, color, stride, device)\n",
    "        # plot_all_boxes(pred, color)\n",
    "        # IPython.display.display(Image.fromarray(color))\n",
    "        \n",
    "        #Landmark screen is object created to contain \"landmarks\" and \"objects\"\n",
    "        landmark_screen = LandmarkScreen(color_frame=color, depth_frame=depth)\n",
    "        landmark_screen.update_OOI(pred) #Update object detections\n",
    "        \n",
    "        #Get intrinsic and extrinsic matrices\n",
    "        intrinsic_mat = get_rs_intrinsic_mat() \n",
    "        inv_intrinsic_mat = np.linalg.inv(intrinsic_mat)\n",
    "        extrinsic_mat = get_rs_extrinsic_mat(type = 'cam2world')\n",
    "        \n",
    "        #Does what is said\n",
    "        landmark_screen.update_cam_coords(inv_intrinsic_mat)\n",
    "        landmark_screen.update_base_coords(extrinsic_mat)\n",
    "        \n",
    "        #landmark_screen.get_OOI() output the \"sorted\" list of object POI (points of interests). POI contains img_coord, cam_coord, base_coord\n",
    "        bottle = landmark_screen.get_OOI()[0]\n",
    "        print(bottle.img_coord, bottle.cam_coord, bottle.base_coord)\n",
    "        world_coords += np.array(bottle.base_coord)\n",
    "    \n",
    "    return world_coords/sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91252cf5-b8b8-4a06-964f-06f5982257a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pre_grasping_perception_singleObj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpre_grasping_perception_singleObj\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pre_grasping_perception_singleObj' is not defined"
     ]
    }
   ],
   "source": [
    "pre_grasping_perception_singleObj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "436ca3ac-bfdd-4c90-971c-07df747c0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore this\n",
    "def pre_grasping_perception_multipleObjs(sample_size = 5):\n",
    "    #Return sampled measurement of the FIRST object \n",
    "    world_coords = np.zeros(3)\n",
    "    final_list_of_Objs = []\n",
    "    dict_of_Objs_sampled = {}\n",
    "    \n",
    "    #Sampling the measurements multiple times, at the same camera angle 0 degrees\n",
    "    for i in range(sample_size):\n",
    "\n",
    "        #Get frame\n",
    "        color_frame, color, depth_frame, depth = get_cur_rs_frame()\n",
    "        \n",
    "        #Visualize\n",
    "        # color = np.ascontiguousarray(color, dtype=np.uint8)\n",
    "        pred = predict(model, config_dict, color, stride, device)\n",
    "        # plot_all_boxes(pred, color)\n",
    "        # IPython.display.display(Image.fromarray(color))\n",
    "        \n",
    "        #Landmark screen is object created to contain \"landmarks\" and \"objects\"\n",
    "        landmark_screen = LandmarkScreen(color_frame=color, depth_frame=depth)\n",
    "        landmark_screen.update_OOI(pred) #Update object detections\n",
    "        \n",
    "        #Get intrinsic and extrinsic matrices\n",
    "        intrinsic_mat = get_rs_intrinsic_mat() \n",
    "        inv_intrinsic_mat = np.linalg.inv(intrinsic_mat)\n",
    "        extrinsic_mat = get_rs_extrinsic_mat(type = 'cam2world')\n",
    "        \n",
    "        #Does what is said\n",
    "        landmark_screen.update_cam_coords(inv_intrinsic_mat)\n",
    "        landmark_screen.update_base_coords(extrinsic_mat)\n",
    "        \n",
    "        #landmark_screen.get_OOI() output the \"sorted\" list of object POI (points of interests). POI contains img_coord, cam_coord, base_coord\n",
    "        dict_of_Objs_sampled[i] = landmark_screen.get_OOI()\n",
    "        \n",
    "        \n",
    "    for key in dict_of_Objs_sampled.keys():\n",
    "        #Assuming the number of objects do not change, and the order of the objects does not change\n",
    "        if key == 0:\n",
    "            for element in dict_of_Objs_sampled[0]:\n",
    "                final_list_of_Objs.append(POI(img_coord = [0,0], depth = 0)) #TODO: Edit this to OOI\n",
    "                \n",
    "        raise len(dict_of_Objs_sampled[key]) == len(final_list_of_Objs)\n",
    "        for i in range(len(dict_of_Objs_sampled[key])):\n",
    "            final_list_of_Objs[i].img_coord += dict_of_Objs_sampled[key][i]\n",
    "            final_list_of_Objs[i].cam_coord += dict_of_Objs_sampled[key][i]\n",
    "            final_list_of_Objs[i].base_coord += dict_of_Objs_sampled[key][i]\n",
    "        \n",
    "    for i in range(len(final_list_of_Objs)):\n",
    "        final_list_of_Objs[i].img_coord /= sample_size\n",
    "        final_list_of_Objs[i].cam_coord /= sample_size\n",
    "        final_list_of_Objs[i].base_coord /= sample_size\n",
    "    return final_list_of_Objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c1525d0-8e13-4c17-9876-f94b304f7028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf67a92-ecaf-4989-9d90-d00fa4f816a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RE1",
   "language": "python",
   "name": "re1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "79b1328d3e7af2c143ede97c186fcfa3b813c9249ef0bf8aa6d6406eca44b2bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
