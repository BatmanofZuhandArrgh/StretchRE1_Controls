{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33aa1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('./yolov7/')\n",
    "\n",
    "from yolov7.predict import load_model, preprocessing, \\\n",
    "    postprocessing, load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844080a",
   "metadata": {},
   "source": [
    "**Object Detection**: From yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32274f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load config from yaml\n",
    "CONFIG_PATH = './yolov7/predict_config.yaml'\n",
    "config_dict = load_config(CONFIG_PATH)\n",
    "\n",
    "#Read img0 to BGR\n",
    "img0 = cv2.imread('./yolov7/inference/images/image1.jpg')\n",
    "original_shape = img0.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011dc536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR ðŸš€ a0bbdb1 torch 1.12.1+cu102 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 306 layers, 36905341 parameters, 36905341 gradients\n"
     ]
    }
   ],
   "source": [
    "#Load model, edit yolov7.pt path\n",
    "model, stride, device = load_model(config_dict, '../yolov7/yolov7.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f1e0779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[        250          78         633         531     0.96753           0]\n",
      " [          2           5         402         527     0.95566           0]\n",
      " [        196         199         254         418     0.88187          27]\n",
      " [        256         207         299         247     0.42756          55]] (4, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hello-robot/anaconda3/envs/RE1/lib/python3.8/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "#Resize, pad\n",
    "img = preprocessing(config_dict, img0, stride, device) \n",
    "inference_shape = img.shape\n",
    "\n",
    "#Inference\n",
    "pred = model(img, augment=config_dict['augment'])[0] #Shape (1, num_preds, 85)\n",
    "\n",
    "#nms and scale coordinates\n",
    "pred = postprocessing(config_dict, pred, original_shape, inference_shape)\n",
    "\n",
    "#xyxy, conf_score, class\n",
    "print(pred, pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f28a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c736343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from POI.object_of_interest import OOI\n",
    "all_objects = []\n",
    "for i in range(pred.shape[0]):\n",
    "    coord = pred[i, :]\n",
    "    obj = OOI(\n",
    "        img_coord = np.array([(coord[2]-coord[0])/2,(coord[3]-coord[1])/2]), \n",
    "        depth = 0,\n",
    "        obj_class = int(coord[-1]),\n",
    "        obj_atributes = 'None', \n",
    "        bbox = ((coord[0],coord[1]),(coord[2],coord[3])),\n",
    "        conf_score = coord[4],\n",
    "        eid = i\n",
    "    )\n",
    "    all_objects.append(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa2488",
   "metadata": {},
   "source": [
    "*Taken from fairo tutorial and memory module\n",
    "\n",
    "\n",
    "**Memory**\n",
    "\n",
    "Now we have setup a small object detection + deduplication pipeline.\n",
    "\n",
    "But the robot is not storing the information of these objects in it's memory yet.\n",
    "\n",
    "If it doesn't store this information in memory, then when you say \"go to the chair\", it does not know where the chair is (unless the chair is in it's field of view at that given moment).\n",
    "\n",
    "`droidlet` provides a memory system that can store generic metadata. This memory system is used by the Dialog Parser + Task controller to do tasks utilizing context provided by information stored in memory.\n",
    "\n",
    "The memory is backed by an SQL database, and has schemas to represent common semantic information for robots and the environment.\n",
    "\n",
    "Let us first create the default `AgentMemory` object for our Locobot using some pre-baked and thoughtful memory schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cefc3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./memory/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc9f82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory.sql_memory import AgentMemory\n",
    "from memory.robot.loco_memory_nodes import NODELIST\n",
    "\n",
    "SQL_SCHEMAS = [\n",
    "    os.path.join(os.getcwd(), \"memory\", \"base_memory_schema.sql\"),\n",
    "    os.path.join(os.getcwd(), \"memory\", \"robot\",\"loco_memory_schema.sql\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e12fed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = AgentMemory(db_file=\":memory:\", schema_paths=SQL_SCHEMAS, nodelist=NODELIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0af1c4",
   "metadata": {},
   "source": [
    "We can see the types of nodes that can be stored inside the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0de3dd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Task': memory_nodes.TaskNode,\n",
       " 'Chat': memory_nodes.ChatNode,\n",
       " 'Location': memory_nodes.LocationNode,\n",
       " 'Attention': memory_nodes.AttentionNode,\n",
       " 'Triple': memory_nodes.TripleNode,\n",
       " 'Interpreter': memory_nodes.InterpreterNode,\n",
       " 'Set': memory_nodes.SetNode,\n",
       " 'Time': memory_nodes.TimeNode,\n",
       " 'Player': memory_nodes.PlayerNode,\n",
       " 'Self': memory_nodes.SelfNode,\n",
       " 'Program': memory_nodes.ProgramNode,\n",
       " 'NamedAbstraction': memory_nodes.NamedAbstractionNode,\n",
       " 'ReferenceObject': memory_nodes.ReferenceObjectNode,\n",
       " 'DetectedObject': memory.robot.loco_memory_nodes.DetectedObjectNode,\n",
       " 'HumanPose': memory.robot.loco_memory_nodes.HumanPoseNode,\n",
       " 'Dance': memory.robot.loco_memory_nodes.DanceNode,\n",
       " 'BCIDetectedObject': memory.robot.loco_memory_nodes.BCIDetectedObjectNode}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a081bf",
   "metadata": {},
   "source": [
    "Let us store the previously detected objects into memory, using this new memory system.\n",
    "\n",
    "For this, we will use the DetectedObjectNode. A physical object is represented in memory as a DetectedObjectNode, which is thoughtfully annotated with properties such as it's color and it's detected xyz location.\n",
    "\n",
    "As a reminder, in the previous section, we deduplicated the objects detected in the scene, and stored them in the variable previous_objects.\n",
    "\n",
    "Now, we will store these all_objects into the memory. Let us start with storing and retreiving one object, and inspecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b879b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from memory.robot.loco_memory_nodes import Detect\n",
    "from memory.robot.loco_memory_nodes import BCIDetectedObjectNode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53af30df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "memory_ids = []\n",
    "for i in range(len(all_objects)):\n",
    "    memory_id = BCIDetectedObjectNode.create(memory, all_objects[i])\n",
    "    memory_ids.append(memory_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcde519b",
   "metadata": {},
   "source": [
    "Now, let us retreive the `DetectedObjectNode` from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e51d618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DetectedObject id 3, pos (0.0, 0.0, 0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.get_mem_by_id(memory_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737cb33",
   "metadata": {},
   "source": [
    "The memory object is in it's raw packed form, and is not yet converted back to a dict with accessible fields.\n",
    "\n",
    "We can access the detected objects back from memory as dicts using the `get_all` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f345485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<POI.object_of_interest.OOI at 0x7fb58615d1c0>,\n",
       " <POI.object_of_interest.OOI at 0x7fb613ae4670>,\n",
       " <POI.object_of_interest.OOI at 0x7fb613602eb0>,\n",
       " <POI.object_of_interest.OOI at 0x7fb613602fa0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BCIDetectedObjectNode.get_all(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa734f7b",
   "metadata": {},
   "source": [
    "Now, let us store the rest of the detected objects into memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb71b73155e50e0471bb3f6aa8b6532acb978151faf9fdb589f3080a8ce410fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
